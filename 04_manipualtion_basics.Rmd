---
title: "Manipulation Basics"
author: "Chidi"
date: "12/4/2017"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE, echo=TRUE, message=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  comment = "#>",
  collapse = TRUE,
  cache = TRUE)
```

##### Note: Answers to questions are what I think they are at the time of writing them. Would have to come back to check my aswers again. All answers are originally mine. When I use some help for the answers, its for a question I don't know, or to check my own answer, and I'll always reference the resource if I find one.

Some common data manipulation functions are `filter`, `select`, `mutate`, `group_by`, `summarise`, these are usually used when performing data manipulation.  

```{r include=TRUE, warning=FALSE, echo=TRUE, message=FALSE}
library(tidyverse)
library(nycflights13)
```

### Filter  

`filter()` allows us subset observations based on their values. The first argument is the name of the dataset, subsequest argument are the expressions for filtering the data frame. 

```{r}
filter(flights, month == 1, day == 1)
```

When comparing different floating numbers, use the `near` function. Take the following example  

```{r}
sqrt(2) ^ 2 == 2 # returns false, but should be true 
(1/49) * 49 == 1 # returns false too, but should be true
```
The above expressions returned false, but should have been true, this is because, computers use finite precision arithmetic, so every number is an approximation. So to avoid that, use the `near` function   

```{r}
near(sqrt(2) ^ 2, 2) # returns True
near(1/49 * 49, 1) # also returns true
```

Other filtering examples  

```{r}
filter(flights, month == 1 & day == 1)
filter(flights, month == 11 | month == 12) # is similar to the the following below
filter(flights, month %in% c(11, 12)) 
```

Also, we can simplify complicated subsetting using some basic equation rules (De Morgan's Law); 

```
!(x | y) is the same as (!x | !y)
!(x & y) is the same as (!x & !y)
```

So for example  
```{r}
filter(flights, !(arr_delay > 120 | dep_delay > 120)) 
# Or
filter(flights, arr_delay <= 120, dep_delay <= 120)
```

Note that expressions involving **NA** values are always **NA**, for example  

```{r}
x <- NA
y <- NA
x == y # evaluates to NA
4 == NA # returns NA too
```
This is because NA is like saying "we don't know", "Not available", so it could be something, but we don't know its value as it isn't available.  We can check for NA with the `is.na`.  `filter` only includes rows that evaluates to true on the given conditions and excludes False and NA results. If I want to include NA values, I'll have to explicitly ask for them. For example  

```{r}
df <- tibble(x = c(1, NA, 3))
filter(df, x > 1) # returns only 3
filter(df, x > 1 | is.na(x)) # returns NA and 3
```


### Exercises  

##### 1. Find all flights that
###### a. Had an arrival delay of two or more hours
###### b. Flew to Houston (IAH or HOU)
###### c. Were operated by United, American, and Delta
###### d. Departed in summer (July, August, and September)
###### e. Arrived more than two hours late, but didn’t leave late
###### f. Were delayed by at least an hour, but made up over 30 minutes in flight
###### g. Departed between midnight and 6am (inclusive)  

Use `?flights` to get information on the data set, to at least know which variables are appropriate. For example, from there, I got to see that the `carrier` variable is the two letter code of airlines name, and the full names are found in the `airlines` dataset, this helped me answer question 1c.  

```{r cache=TRUE}
ans_a <- filter(flights, arr_delay >= 120)
ans_b <- filter(flights, dest %in% c("IAH", "HOU"))
# from the airlines data set, the code for the airlines in question 1c are UA, AA, and DL. 
ans_c <- filter(flights, carrier %in% c("UA", "AA", "DL"))

# Another way I can answer C would be the following
airlines_flights <- merge(airlines, flights) # this is a new dataframe with all unique variables in both datasets, from there I have the names in the question
ans_c <- filter(airlines_flights, grepl("American|Delta|United", name, ignore.case = TRUE))
ans_d <- filter(flights, month %in% c(7:9)) 
## I can also use between to do the above
ans_d <- filter(flights, between(month, 7, 9))
ans_e <- filter(flights, arr_delay > 120 & dep_delay <= 0)
ans_f <- filter(flights, dep_delay >= 60 & dep_delay - arr_delay > 30)

# time here is represented in 24hrs, so 6am == 6:00am is 600 here, 13:00 is 1300, etc
ans_g <- filter(flights, dep_time >= 2400 | dep_time <= 600)
flights_size = nrow(flights)
```

Based on the above, for flights away from NYC in 2013, **`r round((nrow(ans_a)/flights_size) * 100, 2)`%**  of those flights had an arrival delay of two or more hours. There were **`r nrow(ans_b)`** flights to Houston. Three airlines (United, American, and Delta) operated a total of **`r nrow(ans_c)`** flights from NYC, which was about **`r round((nrow(ans_c)/flights_size) * 100, 2)`%** of all the flights from NYC that year. 
**`r round((nrow(ans_d)/flights_size) * 100, 2)`%** of all flights were made during the summer that year. **`r nrow(ans_e)`** flights arrived over two hours late, but had no departure delay, why? And finally, about **`r round((nrow(ans_g)/flights_size) * 100, 2)`%** of the flights from NYC in 2013 were between midnight and 6am.  

##### 2. Another useful dplyr filtering helper is between(). What does it do? Can you use it to simplify the code needed to answer the previous challenges?  

Between is a shortcut for the condition `x >= left & x <= right`. I used it above as well to geth the result for question 1d (`ans_d`) above.  

##### 3. How many flights have a missing dep_time? What other variables are missing? What might these rows represent? 

```{r warning=FALSE}
missing_dep_time <- flights %>%
  filter(is.na(dep_time)) %>%
  count()

(variables_with_missing_names <- names(which(apply(flights, 2, function(x) any(is.na(x))))))
```

**`r missing_dep_time$n`** flights has missing departure time. 

##### 4. Why is NA ^ 0 not missing? Why is NA | TRUE not missing? Why is FALSE & NA not missing? Can you figure out the general rule? (NA * 0 is a tricky counterexample!)

```{r}
NA ^ 0 # I think because (mathematically), anything to the power of 0 is always 1
NA | TRUE # Could be because OR needs just one condition to be true to evaluate to true
FALSE & NA # Similar to the one above, AND would always evaluate to false given at least one false expression
NA * 0 # Could be because this expression is like saying "Something thats not available zero times", which should result to "something thats not available". Thats my logical answer for this one, may not be completely right. 
```


### Mutate  

Mutating data is about creating new columns from existing columns in the data set. With dplyr's `mutate()`, we can create new variables.  

```{r}
flights %>% 
  mutate(gain = arr_delay - dep_delay,
         hours = air_time / 60,
         speed = distance / air_time * 60,
         gains_per_hour = gain/hours)
```

The above mutate would append the newly created variables to the previous ones, but if I want to take the result of only the newly created variables, I'll use `transmute` instead of `mutate`.  

```{r}
flights %>% 
  transmute(gain = arr_delay - dep_delay,
         hours = air_time / 60,
         speed = distance / air_time * 60,
         gains_per_hour = gain/hours)
```

When creating new variables with mutate or transmute, the function should take be able to work on vectors and return a vector of the the same size. Arithmetic functions like +, -, *, /, etc are all vectorised, and can be used with aggregate functions, which is a vectorised operation, and would return a vector, eg `y / sum(y)`. Modular arithmetic, like integer division `%/%` and `%%` (remainder) are also vectorised, and can be useful as it can help us break integers into pieces. For example;

```{r}
flights %>%
  transmute(dep_time,
            hour = dep_time %/% 100,
            minutes = dep_time %% 100)
```

Logarithms are also very useful transformations for dealing with data of multiple orders of magintude. Using `log2` transformation was recommended in the book (R for Data science), given that they're easy to interprete. 

`lead` and `lag` functions are also useful functions as they allows us refer to leading or lagging values. They can be very useful functions when combined with `group_by`

```{r}
(x = 1:10)
lead(x)
lag(x)
x - lag(x)
x != lag(x)
lag(1:10, 5)
```

Other possible useful functions we might want to use includes cummulative and aggregate functions (like `cumsum`, `cumprod`, dplyr's `cummean`), logical comparisons, ranking (like `min_rank`, `row_number`, `dense_rank`, `percent_rank`, `ntile`, etc)
### Exercises  

##### 1. Currently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight. 

```{r}
flights %>%
  mutate(dep_time_since_midnight = dep_time %% 100 + dep_time %/% 100 * 60,
         sched_dep_time_since_midnight = sched_dep_time %% 100 + sched_dep_time %/% 100 * 60) %>%
  select(contains("dep_time"))
```

##### 2. Compare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it?

```{r}
(air_times <- flights %>%
  mutate(arr_dep_diff = arr_time - dep_time,
         arr_dep_diff_mins = arr_dep_diff %/% 100 * 60 + arr_dep_diff %% 100,
         air_time_mins = air_time) %>%
   select(dep_time, arr_time, air_time_mins, arr_dep_diff_mins)) 
```

I expected to see that the difference between arr_time and dep_time should be equal to the air_time, but this wasn't the case. I couldn't figure out why it was so, but according to a solution by [Jeffery Arnold](https://jrnold.github.io/e4qf/data-transformation.html), the difference is due to difference in timezone between the departure and arrival locations, and so to fix it, we need to account for the time zone differences.  

##### 3. Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?  

```{r}
flights %>%
  select(dep_time, sched_dep_time, dep_delay)
```

I'll expect the sum of `sched_dep_time` and `dep_delay` to equal the `dep_time` for each row. To test this, 

```{r}
to_min <- function(hr_min) {
  hr_min = abs(hr_min)
  hrs_to_min = hr_min %/% 100 * 60
  no_of_mins = hr_min %% 100
  hrs_to_min + no_of_mins
}

to_hr_min <- function(time_in_mins) {
  time_in_mins = abs(time_in_mins)
  hrs = time_in_mins %/% 60
  mins = time_in_mins %% 60
  time = as.integer(paste0(hrs, sprintf("%02d", mins)))
  time %% 2400
}

flights %>%
  select(dep_time, sched_dep_time, dep_delay) %>%
  filter(!is.na(dep_time)) %>%
  mutate(sched_delay_sum = (to_hr_min(to_min(sched_dep_time) + dep_delay)),
         same_as_dep_time = (sched_delay_sum == dep_time) | sched_delay_sum == 0) %>%
  summarise(all_true = mean(same_as_dep_time) == 1)
```

I solved the question above using the approach above. Quite a long approach, after solving, I realized it can be simpler, I followed the approach becuase I think I didn't think through the question properly. Here's an easier approach based on [Jeffery Arnold](https://jrnold.github.io/e4qf/data-transformation.html)'s solution.  

```{r}
time2mins <- function(x) {
  x %/% 100 * 60 + x %% 100
}

flights %>%
  select(dep_time, sched_dep_time, dep_delay) %>%
  mutate(dep_delay2 = time2mins(dep_time) - time2mins(sched_dep_time)) 

# Although this implementation doesn't account for all differences, its an easy fix and I understand why
```

From the example above, I still arrive at the same answers as my own implementation, but mine was unnecessarily more complex, because I did different math. This is interesting for me because it meant I didn't think through the problem well and do my math properly. So its important to keep this in mind, especially following the approach, do it first the simplest way you can, then improve solution and fine tune. An approach to improving could be extracting my solution to an equation, and then simplifying the equation, but still, think about a problem well.  

##### 4. Find the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for min_rank()  

```{r}
flights %>%
  mutate(ranked = min_rank(dep_delay)) %>%
  arrange(-ranked) %>% # arrange by "ranked" in descending order
  top_n(10)
```

I can use different tie methods for ranking functions, like `min`, `first`, `average`, etc (see `?rank`). With `min_rank`, it assigns the lowest rank to the smallest value in list of size N, and the highest rank to the highest value in the list, starting from 1 to N. With `min_rank`, values that are tied (the same value) are assigned to the same rank value. See this example between `min` and `max` rank;

```{r, include = FALSE, echo = FALSE}
rank(c(2,3,4,5,4,4), ties.method = "min")
#> [1] 1 2 3 6 3 3
rank(c(2,3,4,5,4,4), ties.method = "max")
#> [1] 1 2 5 6 5 5
```

The output for *min* rank is based on the minimum value of the ranked values (`seq_along(N)`), and that for *max* is based on the maximum value of the ranked values.  

##### 5. What does 1:3 + 1:10 return? Why?

```{r warning = TRUE}
1:3 + 1:10
```

First, it returns a warning that the *"longer object length is not a multiple of shorter object length"*, because the lengths are not the same, so what it computes returns is like spreading the smaller vector over longer one and adding them together, i.e `c(1,2,3,1,2,3,1,2,3,1) + c(1:10)` in this example, and adding them together index by index.  

##### 6. What trigonometric functions does R provide?

R has trig functions like `cos`, `sin`, `tan`, `acos`, `asin`, etc. I used `?cos` to find all of them as they're on the same page. 




### Grouped summaries with `summarise`  

`summarise` collapses a dataframe into a single row. 

```{r}
summarise(flights, delay = mean(dep_delay, na.rm = TRUE))
```

`summarise` can be very handy when we combine with a grouping transformation like `group_by`. When we group a dataframe by some variable, when we then apply dplyr verbs on the dataframe, it applies it group by group. 

```{r}
flights %>%
  group_by(year, month, day) %>%
  summarise(delay = mean(dep_delay, na.rm = TRUE))
```

In the above, I'm grouping by year, for each year group, I group by month, and for each month group in the year group, I group by day, and finally, I apply a summary function on each group (year by month by day). 

```{r}
flights %>%
  group_by(dest) %>%
  summarise(count = n(),
            dist = mean(distance, na.rm = TRUE),
            delay = mean(arr_delay, na.rm = TRUE)) %>%
  arrange(-dist) %>%
  filter(count > 20, dest != "HNL")
```

The above shows an example of a transformation using the piping operator - `%>%`.   

When doing any kind of aggregation, its always useful to include the count (`n()`), or the count of non-missing values `sum(!is.na(x))`. That way, we can be sure that we're not drawing conclusion on a very small subset of data. Another reason we might want to add the count is because, as data size increases, variation decreases, so we can then start seeing a pattern, by adding the count, this means we can filter out things based on count if necessary. Examples below illustrates these points;

```{r egsummarise}
delayed_by_tailnum <- flights %>% 
  filter(!is.na(arr_delay), !is.na(arr_delay)) %>%
  group_by(tailnum)

delayed_by_tailnum %>%
  summarise(delay = mean(arr_delay)) %>%
  ggplot(mapping = aes(x = delay)) +
  geom_freqpoly(binwidth = 10)
```

From the above, I see that some flights have delays up to 5hrs on average. But lets check further. I can get more insight by looking at a scatterplot of the number of flights vs average delay

```{r dependson = 'egsummarise'}
delayed_by_tailnum %>%
  summarise(delay = mean(arr_delay), count = n()) %>%
  ggplot(mapping = aes(x = count, y = delay)) +
  geom_point(alpha = 1/10)
```

From the above, I see that flights with over high delays are those with lower count, and so this may not be enough to make those variations worth it. In this case, its often useful to filter out the groups with smaller number of observations so I can see the pattern better and less of the extreme variations.

```{r}
delayed_by_tailnum %>%
  summarise(delay = mean(arr_delay), count = n()) %>%
  filter(count > 20) %>%
  ggplot(mapping = aes(x = count, y = delay)) +
  geom_point(alpha = 1/10) 
```

From the above I think I can make better inferences on the flights. 

There're several useful summary functions we can use (including our own functions), **location measures** like `mean`, `median`, **spread measures** like `sd`, `IQR`, `mad`, (IQR and mad are more robust and provide better spread estimates in cases where there're outliers in the dataset), **measures of rank** like `min`, `max`, `quantile(x, .25)`, **position measures** like `first(x)`, `nth(x, 7)`, `last(x)`, there are other summary helper functions too, like I can use `n_distinct` to get the number of unique values in a vector. dplyr also have a function `count`, which can just compute the count of groups, eg  

```{r}
delayed_by_tailnum %>%
  count(tailnum)

## the above being equivalent to 
delayed_by_tailnum %>%
  summarise(n = n())
```

We can also provide a weight variable. For example, I can use this to sum the total number of miles a place in the data set covered  

```{r}
delayed_by_tailnum %>%
  count(tailnum, wt = distance)

## Would be equivalent to

delayed_by_tailnum %>%
  summarise(n = sum(distance))
```

We can also use counts and proportions of logical values. example  

```{r}
x = c(4,5,6,7,7,3,2)
(ld4 = x < 4)
sum(ld4)
mean(ld4)
```
The `sum` above prints 2 because *ld4* is a boolean vector, and in a boolean vector, `FALSE` is 0 and `TRUE` is 1, so it sums the `TRUE`s, so since I have two true values there, that gives me 2. Similar thing applies if I use mean, but the mean would give me the proportion which are true.  

We can remove the grouping using `ungroup`.


### Exercises  

##### 1. Brainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. Consider the following scenarios:
###### a. A `flight` is 15 minutes early 50% of the time, and 15 minutes late 50% of the time.  
###### b. A `flight` is always 10 minutes late.  
###### c. A `flight` is 30 minutes early 50% of the time, and 30 minutes late 50% of the time.  
###### d. 99% of the time a flight is on time. 1% of the time it’s 2 hours late.  
##### Which is more important: arrival delay or departure delay?  

```{r}
flight_delays <- flights %>%
  group_by(flight) %>%
  filter(!is.na(arr_delay)) %>%
  summarise(
    count = n(),
    fifteen_mins_early = round(mean(arr_delay == -15), 2),
    fifteen_mins_late = round(mean(arr_delay == 15), 2),
    thirty_mins_early = round(mean(arr_delay == -30), 2),
    thrty_mins_late = round(mean(arr_delay == 30), 2),
    ten_mins_late = round(mean(arr_delay == 10), 2),
    percent_ontime = round(mean(arr_delay == 0), 2),
    two_hrs_late = round(mean(arr_delay == 120), 2))
# a. 
flight_delays %>%
  filter(fifteen_mins_early == .5, fifteen_mins_late == .5) # no flight 
# b. 
flight_delays %>%
  filter(ten_mins_late == 1) # 5 flights

# c.
flight_delays %>%
  filter(thirty_mins_early == .5, thrty_mins_late == .5) # no flight 

# d
flight_delays %>%
  filter(percent_ontime == .99, two_hrs_late == .01) # no flight
```

I think departure delay is more important. If `is.na(dep_delay) == T`, there's no way to know if there's an arrival delay, i.e `is.na(arr_delay)` would be true. 

```{r}
flights %>%
  select(arr_delay, dep_delay) %>%
  filter(is.na(arr_delay)) %>% ## change to dep_delay to compare the result
  summarise(arr_per = mean(is.na(arr_delay)),
            dep_per = mean(is.na(dep_delay)))
```

Also, I'm hypothesizing that if there's no departure delay, there's not going to be an arrival delay.

```{r}
flights %>%
  select(dep_delay, arr_delay) %>%
  filter(!is.na(dep_delay)) %>%
  filter(dep_delay == 0 & arr_delay != 0 )
```

Turns out my hypothesis is wrong, no departure delay doesn't mean there won't be an arrival delay, so as a traveller, I'll look out against flights that always have arrival delay, meaning its more important than departure delays.  

##### 2. Come up with another approach that will give you the same output as `not_cancelled %>% count(dest)` and `not_cancelled %>% count(tailnum, wt = distance)` (without using count()).  

I did these earlier in the lesson of this exercise :)

```{r}
not_cancelled <- flights %>%
  filter(!is.na(dep_time))

not_cancelled %>%
  count(dest) 

## is equivalent to

not_cancelled %>%
  group_by(dest) %>%
  summarise(n = n())

not_cancelled %>%
  count(tailnum, wt = distance)

## is equivalent to
not_cancelled %>%
  group_by(tailnum) %>%
  summarise(n = sum(distance))
```

##### 3. Our definition of cancelled flights `(is.na(dep_delay) | is.na(arr_delay) )` is slightly suboptimal. Why? Which is the most important column?  

This is because one of the filtering condition catches all cancelled flights for both flights, which is `dep_delay`, because whenever `dep_delay` is NA, `arr_delay` would always be empty, the reverse isn't always true. I tested this earlier in exercise 1.

##### 4. Look at the number of cancelled flights per day. Is there a pattern? Is the proportion of cancelled flights related to the average delay?  

```{r}
cancelled_per_day <- flights %>%
  select(year, month, day, time_hour, dep_delay, arr_delay) %>%
  group_by(year, month, day) %>%
  mutate(cancelled = is.na(dep_delay),
         delayed = arr_delay > 0 | dep_delay > 0) %>%
  summarise(number_of_flights = n(),
            number_of_cancelled = sum(cancelled),
            proportion_cancelled = mean(cancelled),
            average_dep_delay = mean(dep_delay, na.rm = TRUE),
            average_arr_delay = mean(arr_delay, na.rm = TRUE),
            proportion_delayed = mean(delayed, na.rm = TRUE)) 

ggplot(data = cancelled_per_day, 
       mapping = aes(x = number_of_flights, y = number_of_cancelled)) +
  geom_point(position=position_jitter(width = .5, height = .5)) +
  geom_smooth(se = FALSE, method = "loess") +
  xlab("Flights per day") +
  ylab("Cancelled flights per day") +
  theme_minimal()
```

From the above, plotting the number of cancelled flights per day against the number of flights each day, generally, I see that the higher the number of flights there were that day, the higher the number of cancelled flights. So there's a positive relation between the number of cancelled flights and the number of flights that day. This is more obvious when I remove days with fewer flights

```{r}
cancelled_per_day %>%
  filter(number_of_flights >= 700) %>%
  ggplot(mapping = aes(x = number_of_flights, y = number_of_cancelled)) +
  geom_point(position = position_jitter(width=1,height=.5), alpha = 1/4) +
  geom_smooth(method = "loess")
```

Now there's rarely a cancelled flight when the number of flights that day are less than 700.  
To answer the question if the proportion of cancelled flights per day is related to the average delay.  

```{r}
delays_by_day <- flights %>%
  select(year, month, day, dep_delay, arr_delay) %>%
  group_by(year, month, day) %>%
  mutate(cancelled = is.na(dep_delay)) %>%
  summarise(
    count = n(),
    prop_cancelled = mean(cancelled, na.rm = TRUE), 
    dep_average = mean(dep_delay, na.rm = TRUE),
    arr_average = mean(arr_delay, na.rm = TRUE))

delays_by_day %>%
  ggplot(data = ., mapping = aes(x = dep_average, y = prop_cancelled, colour = arr_average)) +
  geom_point(position = position_jitter(width = 1)) +
  scale_colour_gradient(low = "white", high = "black") +
  labs(y = "Proportion cancelled", x = "Average departure delay", colour = "Average Arrivals")
```

We can see that there is a positive relationship between the average delay per day and the proportion of cancelled flights per day, so on days with higher delays (departure or arrival), there's higher proportion of cancelled flights.  

##### 5. Which carrier has the worst delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights %>% group_by(carrier, dest) %>% summarise(n()))   

```{r}
flights_with_fullnames = merge(airlines, flights)
flights_with_fullnames %>%
  group_by(name) %>%
  summarise(count = n(), 
            delays = sum(dep_delay, na.rm = TRUE),
            delay_prop = count/delays) %>%
  arrange(desc(delay_prop))
```

From the above, I see that carriers with high number of flights have a higher accumulative amount of delay. Which wouldn't be fair on them to compare them with others on that. A better view is the `delay_prop` variable there, and from there, I see that flights like "US Airways, Hawaiian Airlines, Alaska Airlines" perform worse on the amount of delay per number of flights. Also; 

```{r}
# which airports has the worst delays
airport_names <- airports %>%
  filter(faa %in% c("LGA", "JFK", "EWR")) %>%
  select(faa, name) %>%
  rename(origin = faa, airport = name)

flights_with_fullnames <- merge(airport_names, flights_with_fullnames)
amount_of_delay = sum(flights_with_fullnames$dep_delay, na.rm = T)
flights_with_fullnames %>%
  group_by(airport) %>%
  summarise(count = n(),
            delay = sum(dep_delay, na.rm = TRUE),
            proportion = count/delay) %>%
  arrange(desc(proportion))
```

From the above, I see that *La Guardia* airport has the worse delay per minute flight. This is understandable because they had most of the flights in the dataset, which would cause more accumulative delays, which would mean a bigger proportion of the total delay.  

##### 6. What does the sort argument to count() do. When might you use it?  

Simple example  

```{r}
flights %>%
  group_by(tailnum) %>%
  count(sort = TRUE)
```

The `sort` argument in count sorts the result of count in descending order of `n`. I'll use it if I want the result of `count` to be ordered in decreasing order.  


